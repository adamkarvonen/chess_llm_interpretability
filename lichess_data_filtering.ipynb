{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from datasets import load_dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data begins as a bunch of PGN transcripts. However, to work in tensors we need all transcripts to be the same length. So, this file takes our PGNs and performs some filtering.\n",
    "\n",
    "This notebook has a very similar counterpart, `stockfish_data_filtering.ipynb`. The lichess and stockfish datasets have a slightly different structure and different column names. For most peoples' needs, the lichess dataset alone should suffice, so I made two separate notebooks to keep this one simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data/\"\n",
    "prefix = \"lichess_\"\n",
    "\n",
    "input_file = f'{DATA_DIR}lichess_100mb.csv'\n",
    "output_file = f'{DATA_DIR}lichess_100mb_filtered.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we download the dataset if not present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(input_file):\n",
    "    dataset_path = \"adamkarvonen/chess_games\"\n",
    "    file_path = \"lichess_100mb.zip\"\n",
    "    dataset = load_dataset(dataset_path, data_files=file_path)\n",
    "    df = pd.DataFrame(dataset['train'])\n",
    "    df.to_csv(input_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our LLMs need a delimiter token \";\" at the beginning of every PGN string or it won't work as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(input_file)\n",
    "\n",
    "def format_transcript(game: str) -> str:\n",
    "    new_game = ';' + game\n",
    "    return new_game\n",
    "\n",
    "df['transcript'] = df['transcript'].apply(format_transcript)\n",
    "\n",
    "for game in df.head()['transcript']:\n",
    "    print(game)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter all games to be len 365. This means we discard anything under that length. I chose 365 because that's the 50% of df.describe(). I also count the number of moves (with x.split()) and discard anything below the 25th percentile. This makes it easier if I want to do any move based indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_df = df['transcript'].apply(lambda x: len(x))\n",
    "print(len_df.describe())\n",
    "\n",
    "game_length_in_chars = 365\n",
    "\n",
    "# Data setup. All games must have same length. 50% are >= 690 moves. I will discard all games less than 680, and truncate the rest to 680.\n",
    "filtered_df = df[df['transcript'].apply(lambda x: len(x) >= game_length_in_chars)].copy()\n",
    "filtered_df.loc[:, 'transcript'] = filtered_df['transcript'].apply(lambda x: x[:game_length_in_chars])\n",
    "\n",
    "len_df = filtered_df['transcript'].apply(lambda x: len(x))\n",
    "print(len_df.describe())\n",
    "\n",
    "move_count_df = filtered_df['transcript'].apply(lambda x: len(x.split()))\n",
    "move_count = move_count_df.describe()\n",
    "print(\"move count\", move_count_df.describe())\n",
    "quarter_percentile = move_count['25%']\n",
    "print(\"quarter percentile\", quarter_percentile)\n",
    "\n",
    "# Now I need to filter out games that are too short. I will discard all games less than 25th percentile  moves.\n",
    "filtered_df = filtered_df[filtered_df['transcript'].apply(lambda x: len(x.split()) >= quarter_percentile)]\n",
    "print(filtered_df.describe())\n",
    "print(filtered_df.head())\n",
    "\n",
    "filtered_df.to_csv(output_file, index=False)\n",
    "\n",
    "move_count_df = filtered_df['transcript'].apply(lambda x: len(x.split()))\n",
    "print(move_count_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(filtered_df))\n",
    "print(filtered_df['WhiteElo'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the classification task, I wanted some Elo bins for the probe to classify. This somewhat arbitrarily creates 6 different Elo bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Function to create binned columns and bin index columns\n",
    "def create_binned_columns(df, column_name):\n",
    "    binned_column_name = f'{column_name}Binned'\n",
    "    bin_index_column_name = f'{column_name}BinIndex'\n",
    "    \n",
    "    # Create quantile-based bins\n",
    "    num_bins = 6\n",
    "    # Create quantile-based bins with range labels, dropping duplicates if necessary\n",
    "    df[binned_column_name], bins = pd.qcut(df[column_name], q=num_bins, retbins=True, duplicates='drop')\n",
    "\n",
    "    # Convert bin labels to strings and assign to the column\n",
    "    df[binned_column_name] = df[binned_column_name].apply(lambda x: f'({x.left}, {x.right}]')\n",
    "\n",
    "    # Create bin index column\n",
    "    df[bin_index_column_name] = pd.qcut(df[column_name], q=num_bins, labels=False, duplicates='drop')\n",
    "\n",
    "# Apply the function to both WhiteElo and BlackElo\n",
    "create_binned_columns(filtered_df, 'WhiteElo')\n",
    "create_binned_columns(filtered_df, 'BlackElo')\n",
    "\n",
    "filtered_df.to_csv(output_file, index=False)\n",
    "\n",
    "# Plotting\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(10, 8))\n",
    "\n",
    "# Histogram for WhiteElo\n",
    "axes[0].hist(filtered_df['WhiteElo'], bins=30, color='blue', alpha=0.7)\n",
    "axes[0].set_title('WhiteElo Distribution')\n",
    "axes[0].set_xlabel('WhiteElo')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# Bar chart for WhiteEloBinned\n",
    "bin_counts = filtered_df['WhiteEloBinned'].value_counts()\n",
    "axes[1].bar(bin_counts.index.astype(str), bin_counts.values, color='green', alpha=0.7)\n",
    "axes[1].set_title('WhiteElo Binned Distribution')\n",
    "axes[1].set_xlabel('WhiteElo Bins')\n",
    "axes[1].set_ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_df['WhiteEloBinned'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle all rows of the dataset\n",
    "\n",
    "df = pd.read_csv(output_file)\n",
    "df = df.sample(frac=1, random_state=200).reset_index(drop=True)\n",
    "df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(output_file)\n",
    "\n",
    "print(len(df))\n",
    "\n",
    "# Split df into a train and test split\n",
    "train = df.sample(frac=0.9, random_state=200)\n",
    "test = df.drop(train.index)\n",
    "\n",
    "print(len(train))\n",
    "print(len(test))\n",
    "\n",
    "# Save the train and test splits to csv\n",
    "train.to_csv(f'{DATA_DIR}{prefix}train.csv', index=False)\n",
    "test.to_csv(f'{DATA_DIR}{prefix}test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "othello",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
